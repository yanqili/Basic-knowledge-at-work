{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近在看到神经网络损失函数为什么要用交叉熵的时候，\n",
    "\n",
    "顺便把熵、交叉熵、KL散度、互信息等概念理顺了一遍，\n",
    "\n",
    "原来总是或多或少有些迷糊，这次总算是理清了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考资料:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、物理学博士 傅渥成的知乎live:[熵：时间、信息和生命](https://www.zhihu.com/lives/773839432754151424)\n",
    "\n",
    "讲的很全且有深度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、[交叉熵代价函数（作用及公式推导）](https://blog.csdn.net/u014313009/article/details/51043064)\n",
    "\n",
    "作者用实例说明了在激活函数选用sigmoid时，用MSE当损失函数时，当误差大的时候，参数迭代的慢，而用交叉熵，会在误差大的时候，参数迭代快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "描述事件的混乱程度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/yanqili/Basic-knowledge-at-work/blob/master/Related_images_and_documents/%E7%9F%A5%E4%B9%8Elive_%E7%86%B5/9.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实用$W_n$就可以表示信息量，但是取log后，就可以做加法了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们观察一个随机变量X，X可以等于[1,2,3]，分别的概率是[1/2,1/4,1/4]\n",
    "\n",
    "求X的熵，其实就是求能用多少个bit来表示X？\n",
    "\n",
    "用1个bit(log2(1/1/2) = 1)就能表示X=1\n",
    "\n",
    "用2个bit(log2(1/1/4) = 2)就能表示X=2或者X=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么最公平的方法，就是我们求一个表示X的bit数的期望：\n",
    "$H(X)=\\sum_{i} p(i) * \\log \\frac{1}{p(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一般熵不是单独使用的，得有比较才有使用价值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/yanqili/Basic-knowledge-at-work/blob/master/Related_images_and_documents/%E7%9F%A5%E4%B9%8Elive_%E7%86%B5/10.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、互信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们想知道随机变量X和Y是否相互独立，就需要看互信息了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先假设X、Y相互独立，算出H1 = $-\\sum_{x, y} p(x, y) \\log (p(x) p(y))$\n",
    "\n",
    "再算出X、Y实际熵，H2 = $\\sum_{x, y} p(x, y) \\log (p(x, y))$\n",
    "\n",
    "互信息 I(X;Y) = H1 - H2 ;\n",
    "当H1 = H2 时，X、Y相互独立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 互信息--feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般而言，当X一定时，I(X;Y)越大，表示Y的不确定度越小；或者当Y一定时，I(X;Y)越大，表示X的不确定度越小\n",
    "\n",
    "所以有人用互信息来做特征选择，但是这是有问题的，如[关于互信息的一些注记](https://www.douban.com/note/621588501/)中所说：\n",
    "\n",
    ">互信息提供的是指标和标签之间的绝对信息量，而这在实践中随指标集和标签集的自身规模变化而变化，并不真实反映指标集相对于标签集整体的表达能力\n",
    "\n",
    "必须做类似归一化的操作（normalized mutual information），才能用来筛选特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设2个分布是P、Q\n",
    "\n",
    "H(P,Q) = $-\\sum_{x} p(x) \\log q(x)$\n",
    "\n",
    "就是用分布Q的编码来编码来自P的样本的期望bit数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵--用途之一是多分类问题（二分类属于特例）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    假设神经网络的输出层是3个神经元，意味着有三种分类：100、010、001，每个样本从网络中输出的三个值是分布Q，其实际类别对应的三个数是分布P，上面公式里的x对应这里的三个输出。H(P,Q)越小，越说明P、Q分布接近，越说明这个网络能很好的预测该样本。\n",
    "    如果样本数为n，那么我们需要去最小化：\n",
    "min L = $-\\sum_{j}^n\\sum_{i}^3 p(x_i) \\log q(x_i)$\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 那么问题来了，怎么保证最后一层的输出是个概率分布呢？用softmax，将最后一层的输出转换成概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$softmax \\left(\\mathrm{x}_{i}\\right)=\\mathrm{x}_{i}^{\\prime}=\\frac{e^{\\mathrm{x}_{i}}}{\\sum_{i=1}^{3} e^{\\mathrm{x}_{i}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 还有从最大熵角度来推导的，可以看统计学习方法这本书，不过本人认为从分布相似的角度来理解比较容易记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外这里涉及到，当用神经网络来做分类问题时，若激活函数是sigmoid,损失函数是MSE,那么就会容易出现梯度饱和，换成交叉熵就能解决这个问题。\n",
    "\n",
    "又想到，那做回归问题怎么办呢？回归问题的损失函数不能是交叉熵，这块，我觉得要么改激活函数为RELU，要么不用神经网络做回归，用树模型来做回归，比如xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、相对熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相对熵可以衡量两个随机分布之间的距离\n",
    "\n",
    "$\\begin{aligned} D_{\\mathrm{KL}}(P \\| Q) &=-\\sum_{x} p(x) \\log q(x)+\\sum_{x} p(x) \\log p(x) \\\\ &=H(P, Q) \\quad-\\quad H(P) \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实感觉，核心就是，关键词要考虑2个因素：本文中出现频率、全部语料库中出现频率。TF-IDF就是把这两个因素用乘法综合成了一个指标来选关键词\n",
    "\n",
    "其实还可以做加法、权重加法等，个人这么认为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/yanqili/Basic-knowledge-at-work/blob/master/Related_images_and_documents/%E7%9F%A5%E4%B9%8Elive_%E7%86%B5/15.jpg?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
